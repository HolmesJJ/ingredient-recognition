{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe8b299",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/cifar-100-transfer-learning-using-efficientnet-ed3ed7b89af2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ef186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 10:16:31.496159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.utils import load_img\n",
    "from keras.utils import img_to_array\n",
    "from keras.models import load_model\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb6687ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES_PATH = \"categories.csv\"\n",
    "CATEGORY_PATH = \"dataset_sam/category_id.txt\"\n",
    "TEST_PATH = \"dataset/test/\"\n",
    "TEST_TOP10_PATH = \"dataset/test_top10/\"\n",
    "TEST_LAST10_PATH = \"dataset/test_last10/\"\n",
    "TEST_SAM_PATH = \"dataset_sam/test/\"\n",
    "TEST_DIRS = glob.glob(\"dataset/test/*\")\n",
    "TEST_TOP10_DIRS = glob.glob(\"dataset/test_top10/*\")\n",
    "TEST_LAST10_DIRS = glob.glob(\"dataset/test_last10/*\")\n",
    "\n",
    "MODELS = [\"food-seg-103-xception\", \"food-seg-103-densenet121\", \"food-seg-103-densenet201\"]\n",
    "CHECKPOINT_PATHS = [\"checkpoints/\" + MODEL + \".h5\" for MODEL in MODELS]\n",
    "MODEL_PATHS = [\"models/\" + MODEL + \".h5\" for MODEL in MODELS]\n",
    "\n",
    "IMAGE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa366c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(pd.read_csv(CATEGORIES_PATH, header=None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d99b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes():\n",
    "    cats = pd.read_csv(CATEGORY_PATH, sep=\"\\t\", names=[\"id\", \"name\"])\n",
    "    ids = cats[\"id\"].to_list()\n",
    "    classes = cats[\"name\"].to_list()\n",
    "    classes = [cls.strip() for cls in classes]\n",
    "    return ids, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1395777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_top5(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216ad326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    loaded_models = []\n",
    "    for CHECKPOINT_PATH in CHECKPOINT_PATHS:\n",
    "        loaded_models.append(load_model(CHECKPOINT_PATH, custom_objects={\"acc_top5\": acc_top5}))\n",
    "    print(\"Models Loaded\")\n",
    "    return loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da5a10eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Loaded\n"
     ]
    }
   ],
   "source": [
    "models = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f0a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(row, reduction=\"mean\"):\n",
    "    max_freq_predictions = []\n",
    "    predictions = row.iloc[:len(MODELS)]\n",
    "    label_counts = row.iloc[len(MODELS):]\n",
    "    max_freq_labels = label_counts[label_counts == label_counts.max()].index.tolist()\n",
    "    for max_freq_label in max_freq_labels:\n",
    "        scores = []\n",
    "        for prediction in predictions:\n",
    "            label, score = prediction\n",
    "            if label == max_freq_label:\n",
    "                scores.append(score)\n",
    "        if reduction == \"mean\":\n",
    "            max_freq_score = 0 if len(scores) == 0 else sum(scores) / len(scores)\n",
    "        else:\n",
    "            max_freq_score = 0 if len(scores) == 0 else max(scores)\n",
    "        max_freq_predictions.append([max_freq_label, max_freq_score])\n",
    "    max_freq_prediction = max(max_freq_predictions, key=lambda item: item[1])\n",
    "    return {max_freq_prediction[0]: max_freq_prediction[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4db3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, filepath, top_n=None):\n",
    "    test_image = load_img(filepath, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "    test_image_array = img_to_array(test_image)\n",
    "    test_image_array = np.expand_dims(test_image_array, axis=0)\n",
    "    test_image_array = test_image_array / 255.\n",
    "    predictions = model.predict(test_image_array, verbose=0)\n",
    "    if top_n:\n",
    "        predicted_labels = np.argsort(predictions[0])[::-1][:top_n]\n",
    "    else:\n",
    "        predicted_labels = np.argsort(predictions[0])[::-1]\n",
    "    predicted_scores = predictions[0][predicted_labels]\n",
    "    return predicted_labels, predicted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dfe4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(filepath, top_n_predictions=None, ensemble_type=1, reduction=\"mean\"):\n",
    "    \"\"\"\n",
    "        Ensemble prediction allow filepath or base64, and contains 2 types:\n",
    "\n",
    "        Parameters:\n",
    "        ensemble_type (int): 1 or 2.\n",
    "            For 1: top_n_predictions is available.\n",
    "            For 2: reduction is available.\n",
    "        reduction (str): mean or max\n",
    "    \"\"\"\n",
    "    if ensemble_type == 1:\n",
    "        predictions = {}\n",
    "        for model in models:\n",
    "            predicted_labels, predicted_scores = predict(model, filepath)\n",
    "            for i, label in enumerate(predicted_labels):\n",
    "                if categories[predicted_labels[i]] in predictions:\n",
    "                    if predictions[categories[predicted_labels[i]]] < predicted_scores[i]:\n",
    "                        predictions[categories[predicted_labels[i]]] = predicted_scores[i]\n",
    "                else:\n",
    "                    predictions[categories[predicted_labels[i]]] = predicted_scores[i]\n",
    "        if top_n_predictions:\n",
    "            predictions = dict(sorted(predictions.items(), key=lambda item: item[1], reverse=True)[:top_n_predictions])\n",
    "        else:\n",
    "            predictions = dict(sorted(predictions.items(), key=lambda item: item[1], reverse=True))\n",
    "    else:\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(models):\n",
    "            predicted_labels, predicted_scores = predict(model, filepath, top_n=1)\n",
    "            predictions[MODELS[i]] = [[categories[predicted_labels[0]], predicted_scores[0]]]\n",
    "        predictions = pd.DataFrame(predictions)\n",
    "        predictions.columns = [\"Prediction\" + str(i + 1) for i in range(predictions.shape[1])]\n",
    "        label_counts = predictions.apply(lambda row: pd.Series([item[0] for item in row]).value_counts(), axis=1).fillna(0)\n",
    "        predictions = pd.concat([predictions, label_counts], axis=1)\n",
    "        predictions = list(predictions.apply(lambda row: get_label(row, reduction), axis=1))[0]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e905bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 9832/9832 [7:04:35<00:00,  2.59s/it, accuracy=0.679]\n"
     ]
    }
   ],
   "source": [
    "test_size = 0\n",
    "for test_dir in TEST_DIRS:\n",
    "    name = os.path.basename(test_dir)\n",
    "    for image in glob.glob(TEST_PATH + name + \"/*\"):\n",
    "        test_size += 1\n",
    "\n",
    "running_correct = 0\n",
    "running_size = 0\n",
    "with tqdm(total=test_size) as pbar:\n",
    "    for test_dir in TEST_DIRS:\n",
    "        name = os.path.basename(test_dir)\n",
    "        for img_path in glob.glob(TEST_PATH + name + \"/*\"):\n",
    "            predictions = ensemble_predict(img_path, top_n_predictions=1, ensemble_type=2, reduction=\"mean\")\n",
    "            predicted_name = list(predictions.keys())[0]\n",
    "            if predicted_name == name:\n",
    "                running_correct += 1\n",
    "            running_size += 1\n",
    "            pbar.set_postfix({\"accuracy\": float(running_correct) / running_size})\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83f476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_channels_to_labels(filepath):\n",
    "    labels = []\n",
    "    ids, classes = get_classes()\n",
    "    unique_channels = []\n",
    "    image = cv2.imread(filepath)\n",
    "    num_channels = image.shape[2]\n",
    "    for i in range(num_channels):\n",
    "        channel = image[:, :, i]\n",
    "        unique_values = np.unique(channel)\n",
    "        unique_channels = unique_channels + list(unique_values)\n",
    "        unique_channels = list(np.unique(np.asarray(unique_channels)))\n",
    "    for channel in unique_channels:\n",
    "        if channel != 0:\n",
    "            labels.append(classes[channel])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47e3646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f_score(predicted_labels, true_labels, beta=1):\n",
    "    predicted_labels = set(predicted_labels)\n",
    "    true_labels = set(true_labels)\n",
    "    tp = len(predicted_labels.intersection(true_labels))\n",
    "    fp = len(predicted_labels - true_labels)\n",
    "    fn = len(true_labels - predicted_labels)\n",
    "    tn = len(predicted_labels.union(true_labels)) - tp - fp - fn\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    recall = tp / (tp + fn + 1e-12)\n",
    "    f_score = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
    "    return f_score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98a97b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████| 105/105 [47:06<00:00, 26.92s/it, f=0.364, p=0.49, r=0.328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_score: 0.36378066378018736 precision: 0.4902721088432567 recall: 0.3282766439908135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_size = 0\n",
    "for f in os.listdir(TEST_SAM_PATH + \"img\"):\n",
    "    f_path = os.path.join(TEST_SAM_PATH + \"img\", f)\n",
    "    if os.path.isdir(f_path):\n",
    "        test_size += 1\n",
    "\n",
    "threshold = 0.7\n",
    "f_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "with tqdm(total=test_size) as pbar:\n",
    "    for f in os.listdir(TEST_SAM_PATH + \"img\"):\n",
    "        f_path = os.path.join(TEST_SAM_PATH + \"img\", f)\n",
    "        if os.path.isdir(f_path):\n",
    "            true_labels = convert_channels_to_labels(TEST_SAM_PATH + \"mask/\" + os.path.basename(f_path) + \".png\")\n",
    "            predicted_labels = []\n",
    "            for image_name in os.listdir(f_path):\n",
    "                if \"mask\" not in image_name:\n",
    "                    img_path = os.path.join(f_path, image_name)\n",
    "                    predictions = ensemble_predict(img_path, top_n_predictions=1, ensemble_type=2, reduction=\"mean\")\n",
    "                    predicted_name = list(predictions.keys())[0]\n",
    "                    predicted_score = list(predictions.values())[0]\n",
    "                    if predicted_score > threshold and predicted_name != \"background\":\n",
    "                        predicted_labels.append(predicted_name)\n",
    "                    predicted_labels = list(set(predicted_labels)) \n",
    "            f_score, precision, recall = calculate_f_score(predicted_labels, true_labels)\n",
    "            f_scores.append(f_score)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            pbar.set_postfix({\"f\": sum(f_scores) / len(f_scores),\n",
    "                              \"p\": sum(precisions) / len(precisions),\n",
    "                              \"r\": sum(recalls) / len(recalls)})\n",
    "            pbar.update(1)\n",
    "print(\"f_score:\", (sum(f_scores) / len(f_scores)),\n",
    "      \"precision:\", (sum(precisions) / len(precisions)),\n",
    "      \"recall:\", (sum(recalls) / len(recalls)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
